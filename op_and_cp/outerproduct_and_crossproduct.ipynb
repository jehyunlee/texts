{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 사건의 발단\n",
    "\n",
    "> 저와 비슷한 지식의 오류를 가진 이들을 위해 다소 장황한 이야기를 적습니다.  \n",
    "> 이 단락은 뛰어넘으셔도 좋습니다.\n",
    "\n",
    "이 사건은 저의 무지와 대한수학회의 부적절한 용어 선정으로 인해 시작되었습니다.\n",
    "\n",
    "![vector_products](./images/vector_products.png)\n",
    "\n",
    "수학을 멀리한 저는 벡터간 연산을 **내적**, **외적** 크게 두 갈래로 이해하고 (+**인자간 연산**), 내적과 외적은 같은 연산이 여러 이름을 가지고 있다고 알고 있었습니다. 그런데 `numpy`의 `vectorization`을 연습하는 과정에서 마주친 `numpy.outer()` 연산이 제가 알고 있는 그 외적이 아니더군요.  \n",
    "간혹 machine learning 관련 글을 읽으며 나오는 `outer product`를 머릿속에서 `cross product`로 상상하며 이해해 왔기 때문에 충격이 적지 않았습니다.    \n",
    "<br>  \n",
    "\n",
    "* 내가 알던 외적 = Cross Product,  `numpy.cross()`.  결과물 = **vector**. \n",
    "<br>  \n",
    "\n",
    "    $\n",
    "\\begin{equation}\\mathbf{a} \\times \\mathbf{b} = \\begin{bmatrix}\\mathbf{i}&\\mathbf{j}&\\mathbf{k} \\\\a_{1}&a_{2}&a_{3} \\\\b_{1}&b_{2}&b_{3} \\\\\\end{bmatrix} = (a_{2}b_{3}-a_{3}b_{2})\\mathbf{i} + (a_{3}b_{1}-a_{1}b_{3})\\mathbf{j} + (a_{1}b_{2}-a_{2}b_{1})\\mathbf{k}\\end{equation}\n",
    "$\n",
    "<br>  \n",
    "<br>  \n",
    "\n",
    "* 처음 본 외적 = Outer Product,  `numpy.outer()`.  결과물 = **matrix**. \n",
    "<br>  \n",
    "\n",
    "    $\n",
    "\\begin{equation}\\mathbf{a} \\otimes \\mathbf{b} = \\mathbf{a}\\mathbf{b}^\\top = \\begin{bmatrix}a_{1}\\\\a_{2}\\\\a_{3}\\end{bmatrix}\\begin{bmatrix}b_{1}&b_{2}&b_{3}\\end{bmatrix}=\\begin{bmatrix}a_{1}b_{1}&a_{1}b_{2}&a_{1}b_{3}\\\\a_{2}b_{1}&a_{2}b_{2}&a_{2}b_{3}\\\\a_{3}b_{1}&a_{3}b_{2}&a_{3}b_{3}\\\\ \\end{bmatrix}\\end{equation}\n",
    "$\n",
    "<br><br>  \n",
    "\n",
    "* 그리고 대한수학회에서는 서로 다른 이 둘을 모두 `외적`으로 번역하고 있습니다.\n",
    "\n",
    "![kms](./images/kms_term.PNG)\n",
    "<br>  \n",
    "\n",
    "\"왜 다른 연산을 같은 이름으로 부르지? 내가 알고 있던 건 뭐지?\" 라는 멘탈 데미지를 입고 [wikipedia를 중심으로 구글링을 한 결과](https://ko.wikipedia.org/wiki/%EC%99%B8%EC%A0%81), 사실은 둘이 같은 것이라는 설명을 보고 2차 데미지를 입습니다.  \n",
    "<br>  \n",
    "\n",
    "![wiki_kor](./images/wiki_kor01.PNG)\n",
    "<br>  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cross Product vs Outer Product\n",
    "\n",
    "> * [딥러닝하는 의사](https://www.youtube.com/channel/UC2OC6sqKOSLNL4S6g1L3SeA)님께서 친절한 설명을 주셨습니다.  \n",
    "> `Cross Product`와 `Outer Product`는 비슷하지만 본질적으로 다르다고 합니다.  \n",
    "> <br>   \n",
    "> * 그 외 아래 레퍼런스를 참고했습니다.  \n",
    "> [jjycjn's Math Storehouse](https://jjycjnmath.tistory.com/537)  \n",
    "> [MATH3MA](https://www.math3ma.com)  \n",
    "> [StackExchange Mathematics](https://math.stackexchange.com)  \n",
    "> [Wolfram Mathworld](http://mathworld.wolfram.com/)  \n",
    "> [YBIGTA님 github](https://github.com/YBIGTA)\n",
    "<br>  \n",
    "  \n",
    "#### 1.0. 문제 재정의  \n",
    "* `Cross Product`를 일반화하면 `Exterior Product`가 됩니다. (Euclidean → non-Euclidean)\n",
    "* `Outer Product`는 두 vector의 `Tensor Product`를 행렬로 표현한 것입니다.  \n",
    "\n",
    "`Tensor Product`와 `Exterior Product`를 간략히 공부해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Tensor Product\n",
    "\n",
    "* 두 개의 기존 자원이 있을 때, 이를 포함한 새로운 자원을 만들고자 할 수 있습니다.  \n",
    "  - 두 정수가 있다면 최소공배수와 최대공약수를 얻을 수 있고,  \n",
    "  - 두 집합이 있다면 곱집합(Cartesian Product : $\\{1, 2\\} \\times \\{3, 4\\} = \\{(1,3), (1,4), (2,3), (2,4)\\}$)이나 합집합을 만들 수 있습니다.  \n",
    "  - 두 그룹이 있다면, 직합(direct sum)이나 자유곱(free product)을 할 수 있습니다.  \n",
    "<br>\n",
    "* 두 벡터 공간 $V$와 $W$가 있을 때, 이 두 벡터 공간의 정보를 모두 담고 있는 새로운 벡터 공간을 만들 수 있습니다. \n",
    "  - 이 때 `Tensor Product`를 사용합니다.  \n",
    "<br>\n",
    "* 우리는 '벡터 공간은 기저(basis)들로 결정된다'는 직관을 가지고 있습니다.  \n",
    "따라서 기저들의 정보를 합쳐주면 새로운 벡터공간에 대한 모든 정보를 알 수 있게 됩니다.  \n",
    "예를 들어, (1, 0, 0), (0, 1, 0), (0, 0, 1)이라는 3차원 벡터공간과 (1, 0), (0, 1) 이라는 2차원 벡터공간이 있을 때 이들의 정보를 모두 유지하면서 새로운 벡터공간을 만들고 싶을 때 쓰는 게 `Tensor Product` 입니다.  \n",
    "<br>\n",
    "* 서로 다른 차원의 벡터공간들을 어떻게 합칠 수 있을까요?  선택지는 **행렬**밖에 없습니다.  \n",
    "  우리가 가진 도구가 많지 않아서 발생하는 일입니다.  \n",
    "<br> \n",
    "$V$와 $W$에 각기 $\\mathbf{v} = \\begin{bmatrix}v_{1}\\\\v_{2}\\\\\\vdots\\\\v_{n}\\end{bmatrix}, \\mathbf{w} = \\begin{bmatrix}w_{1}\\\\w_{2}\\\\\\vdots\\\\w_{m}\\end{bmatrix}$ 이라는 두 행렬이 있고, $\\mathbf{v} = v_{1}\\mathbf{e_{1}} + v_{2}\\mathbf{e_{2}} + \\cdots + v_{n}\\mathbf{e_{n}}$ 일 때   \n",
    "<br> \n",
    "$\n",
    "\\mathbf{v}\\otimes\\mathbf{w} = \\begin{bmatrix}v_{1}w_{1}&v_{1}w_{2}&\\cdots&v_{1}w_{m}\\\\v_{2}w_{1}&v_{2}w_{2}&\\cdots&v_{2}w_{m}\\\\\\vdots&\\vdots&\\ddots&\\vdots\\\\v_{n}w_{1}&v_{n}w_{2}&\\cdots&v_{n}w_{m}\\\\\\end{bmatrix}\n",
    "$ 으로 정의됩니다.  \n",
    "<br>  \n",
    "\n",
    "  따라서 $\\text{dim}(V\\otimes W) = \\text{dim}(V)\\times\\text{dim}(V)$ 가 됩니다.  \n",
    "  서로 다른 벡터 두 개를 연산해야 하니 덧셈은 사실상 불가능하고, 서로 독립인 행과 열을 가진 행렬로만 표현이 가능한 것입니다.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Exterior Product\n",
    "\n",
    "* `Exterior Product`의 시작점은 벡터공간 $V$와 $W$가 있을 때, 이들로부터 만들어낸 `Tensor Product` $V\\otimes W$ 입니다.  \n",
    "<br>  \n",
    "* 먼저 `quotient`이라는 개념을 이해해 봅시다.  Exterior product가 quotient로 정의되기 때문입니다.\n",
    "  - \"equivalent relation $\\text{~}$ 에 대해서 집합 $\\text{X}$를 $\\text{X/~}$로 잘라준 것\" 입니다. (어려워도 조금만 더 읽어주세요)  \n",
    "  - 직관적인 예시로 '24시간 법'이 있습니다. 15시에서 10시간이 지나면 1시가 됩니다.  \n",
    "  - 이것이 quotient의 전부입니다. 우리는 24를 0으로 보겠다고 약속을 한 것이고, 그 이외의 연산은 모두 동일합니다.  \n",
    "  - `mod`와 같습니다. 이걸 우리는 $\\text{Z/24Z}$, 또는 $\\text{Z/~}$로 표기합니다.  \n",
    "  - 이 때 사용된 $\\text{~}$는 [`equivalence relation`](http://mathworld.wolfram.com/QuotientSpace.html)입니다.  \n",
    "<br>  \n",
    "* 두 개의 vector space $\\text{V}$로부터 생성되는 tensor product $V\\otimes V$를 생각해 봅시다.  \n",
    "  - 그러면, Exterior algebra $\\wedge(V)$ 는 다음 집합 위에 정의되는 이중선형(bilinear)연산입니다.  \n",
    "      $V\\otimes V/\\text{I, where I} = \\{\\mathbf{v}\\otimes \\mathbf{v}|  \\forall\\mathbf{v} \\text{  in } V \\}$  \n",
    "  - 즉, 이 공간에서는 자기 자신으로 tensor product를 한 녀석들을 전부 0으로 간주하겠다는 뜻입니다.\n",
    "  - 예를 들면, 아래와 같은 행렬들은 모두 0입니다.  \n",
    "      $(1,2) \\otimes (1, 2) = \\begin{bmatrix}1&2\\\\2&4\\end{bmatrix}= 0$  \n",
    "      $(0,1) \\otimes (0, 1) = \\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}= 0$  \n",
    "<br>  \n",
    "\n",
    "  - 왜 0으로 정의했나면, 넓이와 부피 때문에 그렇습니다.  \n",
    "    exterior product는 넓이, 부피, 초부피를 의미합니다.  \n",
    "    그리고, 직관적으로 **하나의 벡터로 생성되는 도형의 부피는 0**이기 때문입니다.  \n",
    "<br>  \n",
    "  \n",
    "* Alternating Albegra입니다.  \n",
    "  - 무슨 말이냐면, $f(\\cdots, x_{i}, \\cdots, x_{j}, \\cdots) = -f(\\cdots, x_{j}, \\cdots, x_{i}, \\cdots)$ 라는 입니다.\n",
    "  - Tensor Product는 $x \\otimes y = -y \\otimes x$ 라는 관계가 있습니다.  \n",
    "      항등식 $(x+y) \\otimes (x+y) = x \\otimes y + y \\otimes x = 0$ 에서 유도할 수 있습니다.  \n",
    "  - Exterior algebra는 biliear이므로 $x \\wedge y = -y \\wedge x$ 도 성립합니다.  \n",
    "  - 마찬가지 이유로 $f(0,0,\\cdots,0)=0$ 일 수 밖에 없어 기함수가 됩니다.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why: 왜 우리가 Tensor Product와 External Product를 생각하는가?\n",
    "\n",
    "* `Tensor Product` : \"두 벡터 공간의 정보를 다 가진 새로운 벡터공간을 만들고 싶다.\"  \n",
    "* `Exterior Product` : \"벡터들로 이루어진 공간의 부피와 넓이를 구하고 싶다.\"  \n",
    "<br>  \n",
    "* 두 벡터로 이루어진 공간이 `Outer Product`이고, 두 벡터로 이루어진 공간의 부피가 `Cross Product`입니다.  \n",
    "* 그리고 `outer product`의 determinant는 0입니다.  \n",
    "  \n",
    "### 3. For What: Tensor Product와 External Product를 어디에 사용하는가?\n",
    "\n",
    "* `Tensor Product` : Deep Learning, Computer Vision에서 제일 잘 쓰이는 예시는 `style transfer`등에 적용되는 `gram matrix`가 있습니다.    \n",
    "  - `gram matrix`에 대한 설명은 다음 [동영상](https://youtu.be/RWro8WzTDSM), [글1](https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f), [글2](https://github.com/YBIGTA/data-science-2018/blob/master/DLCV/2018-02-02-Neural-style-transefer.md)를 참고하시면 좋겠습니다.\n",
    "* `Exterior Product` : 당연히 넓이, 부피 계산에 사용합니다.    \n",
    "<br>    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. `Style Transfer`에서의 `Gram Matrix`  \n",
    "* `outer product`의 사용 예시로, `style transfer`에 적용된 `gram matrix`를 간단히 소개합니다.  \n",
    "> source : [YBIGTA님 github 글](https://github.com/YBIGTA/data-science-2018/blob/master/DLCV/2018-02-02-Neural-style-transefer.md)  \n",
    "\n",
    "**(1) Style Transfer**  \n",
    "- 특정 이미지에서 추출한 스타일을 다른 이미지에 입히는 행위입니다.  \n",
    "![style_transfer](./images/styletransfer.png)  \n",
    "<br>  \n",
    "\n",
    "**(2) Activation Maps**  \n",
    "- 이미지를 `VGG`와 같은 convolutional layers에 통과시키면, 필터 갯수만큼의 새로운 map이 나옵니다.  \n",
    "- 아래는 9개의 필터에서 나온 activation map을 이미지로 복원한 결과입니다.  \n",
    "- 빨간색으로 표시된 부분에서는 수직선이, 파란색 부분에서는 주황색을 탐지하는 것이 보입니다.  \n",
    "![amaps](./images/gram_00.png)  \n",
    "<br>  \n",
    "\n",
    "**(3) Correlations between channels: Gram matrix**  \n",
    "- channel간 correlation을 구하기 위해, 먼저 3D tensor를 unroll해서 matrix로 만듭니다.  \n",
    "- $n_{W} \\times n_{H} \\times n_{C}$ 차원이 $(n_{H} \\times n_{W}) \\times n_{C}$ 로 변환됩니다.  \n",
    "- 이 matrix를 $A$라고 합시다.  \n",
    "![unroll](./images/gram_01.png)  \n",
    "<br>  \n",
    "\n",
    "- channel간 correlation은 $A \\otimes A = AA^{T}$로 표현할 수 있습니다.  \n",
    "- 이를 `Gram matrix`라고 부릅니다.  \n",
    "![gmatrix](./images/gram_02.png)  \n",
    "<br>  \n",
    "\n",
    "- Style Image (S)의 activation map으로부터 gram matrix를 만든 것을 $G^{[l](S)}$, Generation Image(G)를 같은 방법으로 변환한 것을 $G^{[l](G)}$라고 부릅시다.  \n",
    "- 두 `Gram matrix`는 각각 S의 Style, G의 Style 정보를 담고 있다고 볼 수 있습니다.  \n",
    "- 두 matrix가 유사하다면, S와 G는 유사한 스타일을 지녔다고 할 수 있습니다.  \n",
    "- 이에 따라 style cost는 다음과 같의 정의됩니다.  \n",
    "![cost](./images/cost.png)  \n",
    "<br>  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
